{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c643dc8d",
   "metadata": {},
   "source": [
    "# RAG Indexing Notebook\n",
    "\n",
    "This notebook demonstrates RAG (Retrieval-Augmented Generation) pipeline setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc495e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List, Dict, Any\n",
    "import numpy as np\n",
    "\n",
    "# Sample documents for RAG\n",
    "documents = [\n",
    "    {\n",
    "        \"id\": \"spark_tuning_1\",\n",
    "        \"title\": \"Performance Tuning Guide\",\n",
    "        \"content\": \"Spark performance tuning involves optimizing resource allocation, partition count, and shuffle operations.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"spark_partitioning\",\n",
    "        \"title\": \"Partitioning Strategy\",\n",
    "        \"content\": \"Proper partitioning is crucial for Spark performance. Consider data distribution and executor count.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"delta_optimization\",\n",
    "        \"title\": \"Delta Lake Optimization\",\n",
    "        \"content\": \"Delta Lake offers ACID transactions and optimizations like Z-ordering and compaction.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"broadcast_joins\",\n",
    "        \"title\": \"Broadcast Join Strategy\",\n",
    "        \"content\": \"Use broadcast joins when one DataFrame is small enough to fit in memory.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"skew_handling\",\n",
    "        \"title\": \"Handling Data Skew\",\n",
    "        \"content\": \"Data skew can cause performance issues. Use salting or repartitioning to handle it.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Loaded {len(documents)} documents\")\n",
    "for doc in documents:\n",
    "    print(f\"- {doc['title']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63abe6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate document embeddings (in real system, use transformers library)\n",
    "def create_mock_embeddings(documents: List[Dict]) -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Create mock embeddings for documents.\n",
    "    In production, use actual embedding models like sentence-transformers.\n",
    "    \"\"\"\n",
    "    embeddings = {}\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    for doc in documents:\n",
    "        # Create a mock embedding based on document ID\n",
    "        embedding = np.random.randn(384)  # 384-dimensional embedding\n",
    "        embeddings[doc['id']] = embedding\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "embeddings = create_mock_embeddings(documents)\n",
    "print(f\"\\nCreated embeddings for {len(embeddings)} documents\")\n",
    "print(f\"Embedding dimension: {embeddings[list(embeddings.keys())[0]].shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a81feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vector store\n",
    "class SimpleVectorStore:\n",
    "    def __init__(self):\n",
    "        self.documents = {}\n",
    "        self.embeddings = {}\n",
    "    \n",
    "    def add_documents(self, documents: List[Dict], embeddings: Dict):\n",
    "        \"\"\"Add documents and embeddings to store\"\"\"\n",
    "        for doc in documents:\n",
    "            self.documents[doc['id']] = doc\n",
    "            self.embeddings[doc['id']] = embeddings[doc['id']]\n",
    "    \n",
    "    def search(self, query_embedding: np.ndarray, top_k: int = 3) -> List[Dict]:\n",
    "        \"\"\"Search for similar documents\"\"\"\n",
    "        similarities = []\n",
    "        \n",
    "        for doc_id, doc_embedding in self.embeddings.items():\n",
    "            # Cosine similarity\n",
    "            similarity = np.dot(query_embedding, doc_embedding) / (\n",
    "                np.linalg.norm(query_embedding) * np.linalg.norm(doc_embedding)\n",
    "            )\n",
    "            similarities.append((doc_id, similarity))\n",
    "        \n",
    "        # Sort by similarity and return top-k\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        results = []\n",
    "        \n",
    "        for doc_id, similarity in similarities[:top_k]:\n",
    "            doc = self.documents[doc_id].copy()\n",
    "            doc['similarity'] = round(float(similarity), 4)\n",
    "            results.append(doc)\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Initialize and populate vector store\n",
    "vector_store = SimpleVectorStore()\n",
    "vector_store.add_documents(documents, embeddings)\n",
    "print(\"Vector store initialized with documents and embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b463c913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test RAG retrieval\n",
    "# Create a mock query embedding (in production, encode actual query)\n",
    "query = \"How to optimize Spark performance with data skew?\"\n",
    "query_embedding = np.random.randn(384)\n",
    "\n",
    "# Retrieve relevant documents\n",
    "results = vector_store.search(query_embedding, top_k=3)\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"\\nTop {len(results)} Relevant Documents:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"\\n{i}. {result['title']} (similarity: {result['similarity']})\")\n",
    "    print(f\"   Content: {result['content'][:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fa0d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate recommendations using RAG\n",
    "def generate_rag_recommendations(query: str, retrieved_docs: List[Dict]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Generate recommendations based on retrieved documents.\n",
    "    In production, use an LLM like GPT-4 or Claude.\n",
    "    \"\"\"\n",
    "    recommendations = []\n",
    "    \n",
    "    # Extract keywords from documents\n",
    "    keywords = {}\n",
    "    for doc in retrieved_docs:\n",
    "        words = doc['content'].lower().split()\n",
    "        for word in words:\n",
    "            keywords[word] = keywords.get(word, 0) + 1\n",
    "    \n",
    "    # Generate synthetic recommendations\n",
    "    if 'skew' in query.lower():\n",
    "        recommendations.append(\"Use salting technique for join operations\")\n",
    "        recommendations.append(\"Consider repartitioning with even distribution\")\n",
    "        recommendations.append(\"Use adaptive partitioning strategy\")\n",
    "    \n",
    "    if 'partition' in query.lower():\n",
    "        recommendations.append(\"Optimize partition count based on data size\")\n",
    "        recommendations.append(\"Consider date-based partitioning strategy\")\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "recommendations = generate_rag_recommendations(query, results)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Generated Recommendations:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, rec in enumerate(recommendations, 1):\n",
    "    print(f\"{i}. {rec}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65358ee",
   "metadata": {},
   "source": [
    "## RAG Pipeline Summary\n",
    "\n",
    "This notebook demonstrates the Retrieval-Augmented Generation (RAG) pipeline:\n",
    "\n",
    "1. **Document Collection**: Gather Spark documentation\n",
    "2. **Embedding**: Convert documents to embeddings using transformers\n",
    "3. **Vector Store**: Store embeddings in Pinecone/Weaviate\n",
    "4. **Retrieval**: Semantic search for relevant documents\n",
    "5. **Generation**: Use retrieved context to generate recommendations\n",
    "\n",
    "This approach combines the benefits of:\n",
    "- Retrieval-based systems (factual accuracy)\n",
    "- Generative models (natural language generation)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

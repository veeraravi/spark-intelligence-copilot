# README

# Spark Intelligence Copilot

An AI-powered platform for analyzing, monitoring, and optimizing Apache Spark and Databricks jobs.

## Features

âœ¨ **Intelligent Job Analysis**
- Metadata extraction and schema analysis
- Partition strategy optimization
- Data skew detection and mitigation
- Runtime prediction using ML models

ğŸš€ **Performance Optimization**
- Spark configuration recommendations
- Delta Lake-specific optimizations
- Cost analysis and savings estimation
- Hardware and executor tuning

ğŸ¤– **AI-Powered Insights**
- Retrieval-Augmented Generation (RAG) for intelligent recommendations
- Machine learning models for runtime prediction
- Rule-based optimization engines
- Continuous learning from historical data

â˜ï¸ **Cloud Native**
- Built on Google Cloud Platform (GCP)
- Kubernetes-ready deployment
- Scalable microservices architecture
- Enterprise-grade security

## Quick Start

### Local Development

1. **Clone Repository**
```bash
git clone https://github.com/spark-intelligence/copilot.git
cd spark-intelligence-copilot
```

2. **Set Up Environment**
```bash
cp .env.example .env
```

3. **Start Services**
```bash
docker-compose up -d
```

4. **Access API**
```
http://localhost:8000
Documentation: http://localhost:8000/docs
```

### Cloud Deployment

See [Deployment Guide](docs/deployment_guide.md) for comprehensive instructions.

## Project Structure

```
spark-intelligence-copilot/
â”œâ”€â”€ app/                 # FastAPI application
â”œâ”€â”€ agents/              # Specialized analysis agents
â”œâ”€â”€ orchestration/       # Workflow orchestration
â”œâ”€â”€ sources/             # Data source connectors
â”œâ”€â”€ rag/                 # RAG pipeline
â”œâ”€â”€ rules_engine/        # Rule-based optimization
â”œâ”€â”€ ml/                  # Machine learning models
â”œâ”€â”€ storage/             # Data persistence
â”œâ”€â”€ connectors/          # External integrations
â”œâ”€â”€ infra/               # Infrastructure as Code
â”‚   â”œâ”€â”€ terraform/       # GCP infrastructure
â”‚   â”œâ”€â”€ docker/          # Containerization
â”‚   â””â”€â”€ k8s/             # Kubernetes manifests
â”œâ”€â”€ notebooks/           # Jupyter notebooks
â”œâ”€â”€ tests/               # Test suite
â”œâ”€â”€ docs/                # Documentation
â””â”€â”€ requirements.txt     # Python dependencies
```

## Architecture

See [Architecture Guide](docs/architecture.md) for detailed system design.

## API Endpoints

### Analyze Job
```bash
POST /api/v1/analyze/job
```

### Analyze Partition
```bash
POST /api/v1/analyze/partition
```

### Get Recommendations
```bash
GET /api/v1/recommendations/{job_id}
```

### Get Metrics
```bash
GET /api/v1/metrics/{job_id}
```

See [API Documentation](docs/api_docs.md) for full details.

## Technologies

- **Framework**: FastAPI, Uvicorn
- **Database**: PostgreSQL, BigQuery
- **ML**: PyTorch, Scikit-learn
- **Cloud**: Google Cloud Platform
- **Container**: Docker, Kubernetes
- **Infrastructure**: Terraform
- **Message Queue**: Kafka
- **Cache**: Redis
- **Vector Store**: Pinecone

## Configuration

Create a `.env` file based on `.env.example`:

```env
DEBUG=False
DATABASE_URL=postgresql://user:password@localhost/spark_copilot
GCP_PROJECT_ID=your-project-id
KAFKA_BOOTSTRAP_SERVERS=localhost:9092
```

## Development

### Install Dependencies
```bash
pip install -r requirements.txt
```

### Run Tests
```bash
pytest tests/ -v --cov=app --cov-report=html
```

### Code Quality
```bash
# Format code
black .

# Lint
flake8 . --max-line-length=100

# Type checking
mypy .
```

### Run Locally
```bash
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

## Deployment

### Local
```bash
docker-compose up
```

### Cloud (GCP)
```bash
cd infra/terraform
terraform apply -var="gcp_project_id=YOUR_PROJECT_ID"
```

## Contributing

Contributions are welcome! Please follow these steps:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## License

This project is licensed under the MIT License - see LICENSE file for details.

## Support

- ğŸ“§ Email: support@sparkintelligence.com
- ğŸ’¬ GitHub Discussions: [Link to discussions]
- ğŸ› Issue Tracker: [GitHub Issues](https://github.com/spark-intelligence/copilot/issues)

## Acknowledgments

- Apache Spark community
- Databricks documentation
- Contributors and testers

---

**Built with â¤ï¸ by the Spark Intelligence Team**
